# @package _global_

# usage: +experiment=pm/okapi

defaults:
    - override /dm: pm/okapi
    - override /backbone: pm/convnext
    - override /predictor: fcn
    - override /alg: okapi
    - override /logger: ds
    - override /checkpointer: pm
    - _self_

trainer:
  max_steps: 30000
  multiple_trainloader_mode: 'min_size'
  val_check_interval: 1000
  precision: 16

alg:
  lr: 1e-4
  use_test_data: false
  binary: true
  loss_u_weight_end: 1.0
  fixed_caliper_max: 0.9
  std_caliper: 0.2
  temp_ps: 1
  k: 5
  reweight: true
  normalize: true
  loss_u_fn: L2_FEAT
  temp_nnclr: 0.1
  warmup_steps: 3000
  ema_decay_start: 0.996
  ema_decay_end: 1.0
  ema_warmup_steps: ${ trainer.max_steps }
  mb_capacity: 65536
  twoway_caliper: true
  online_ps: true
  scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
  scheduler_kwargs:
    T_max: ${ trainer.max_steps }
    eta_min: 5e-7

logger:
  group: okapi_${alg.loss_u_fn}_binary_${alg.binary}_k_${alg.k}_temp_ps_${alg.temp_ps}_normalize_${alg.normalize}_poverty_map
