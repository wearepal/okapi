# @package _global_

# usage: +experiment=cc/okapi

defaults:
    - override /dm: civil_comments
    - override /backbone: dbert
    - override /predictor: fcn
    - override /alg: okapi
    - override /logger: ds
    - override /checkpointer: cc
    - _self_

backbone:
  version: BASE_UNC

predictor:
  num_hidden: 1
  dropout_prob: 0.1
  activation: GELU
  norm: LN

dm:
  train_batch_size_l: 48
  train_batch_size_u: 48
  use_unlabeled: true
  training_mode: step
  groupby_fields: ['y']
  sampling_groups: ${ dm.groupby_fields }
  max_token_length: 300
  sampling_method: weighted
  seed: 0

trainer:
  max_steps: 20000
  multiple_trainloader_mode: 'min_size'
  val_check_interval: 1000
  precision: 16
  gradient_clip_val: 1.0

alg:
  use_test_data: false

  binary: true
  fixed_caliper_max: 1.0
  std_caliper: 10
  twoway_caliper: true
  online_ps: true
  reweight: true
  normalize: true
  temp_ps: 1
  k: 5

  loss_u_fn: L2_FEAT
  loss_u_weight_start: 0
  loss_u_weight_end: 10
  temp_nnclr: 0.1
  warmup_steps: 2000

  ema_decay_start: 0.996
  ema_decay_end: 0.996
  ema_warmup_steps: ${ trainer.max_steps }
  mb_capacity: 65536

  lr: 5.e-5
  scheduler_cls: torch.optim.lr_scheduler.CosineAnnealingLR
  scheduler_kwargs:
    T_max: ${ trainer.max_steps }
    eta_min: 5e-7

logger:
  group: civil_comments_okapi_${alg.loss_u_fn}_binary_${alg.binary}_k_${alg.k}_temp_ps_${alg.temp_ps}_normalize_${alg.normalize}
